{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c661ac3-36bd-4660-bbab-9b44c9624dd2",
   "metadata": {},
   "source": [
    "# SDV CPAR model training\n",
    "\n",
    "#### Here we add peaks and valleys to the static data (context)\n",
    "\n",
    "- We select the n largest and n smallest values of each time series, this is chosen by setting the variable `peaks`.\n",
    "- We pick also the timestamp for each of those values, and add it to the context (static features).\n",
    "- We leave the time series and use it with the context to train the generator.\n",
    "- The final output is a time series data with a context that has the min and maxes synthesized.\n",
    "\n",
    "Issues:\n",
    "\n",
    "- In the SD, the time when an extrema happens for a particular `datapoint_id` not necessarily matches the extrema on the time series for this `datapoint_id`.\n",
    "- SDV has some problems manipulating datetime data types. The model can be fitted but throws errors when generating the SD. To fix this, we keep only the hour as `str` type, thus this make only sense on hourly data for an specific day.\n",
    "- For some reason the context columns has to be passed in the order they appear in the real data dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c066cd0-5222-40d3-af2a-af0bc2957b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4e0b4c-92b6-44ac-8eec-185000e026f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b455b8b0-3e49-4bf4-89c9-c8d9dc78fd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2965868a-c9a3-4354-b9ca-bd7b41e64c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sdv\n",
    "from sdv.sequential import PARSynthesizer\n",
    "#from sdv.constraints import Unique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90b70f1-4431-4300-80fc-b375ae6445ee",
   "metadata": {},
   "source": [
    "### Inputs\n",
    "- Number of days or datafilename\n",
    "- Epochs\n",
    "- Peaks (number of max and min values of the time series)\n",
    "- Size of sampled synthetic data\n",
    "- Real data file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46bdbf0-1748-489c-8102-f93dc552d739",
   "metadata": {},
   "outputs": [],
   "source": [
    "days = 1\n",
    "data_dir = \"../\"\n",
    "epochs = 128\n",
    "peaks = 1\n",
    "sample_size = None\n",
    "datafilename = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a57ce4-67e4-4345-a981-29a895c7c5a7",
   "metadata": {},
   "source": [
    "### Read real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8034877b-1549-43f0-8f61-05107716fa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "if datafilename:\n",
    "    real_data = pd.read_csv(datafilename, index_col=0)    \n",
    "else:\n",
    "    real_data = pd.read_csv(f\"{data_dir}real_data_sdv_{days}_days.csv\", index_col=0)\n",
    "\n",
    "if not sample_size:\n",
    "    sample_size = len(real_data.datapoint_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b2195b-dcc0-43f0-948b-813c3cf2006e",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1ce0b1-849c-4e29-89c0-2edb407318e7",
   "metadata": {},
   "source": [
    "### Add peaks and valleys\n",
    "\n",
    "Find max and min on the time series and add it to the static columns, keeping time series data AS IS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03288f6a-7392-45c7-98a1-00b5ff34e6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_peaks(df, count=1):\n",
    "    \"\"\" Select max and min values for each time series and add it to a dataframe along with the timestamp\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe with static and time series values.\n",
    "    count : int\n",
    "        The number of max and min values to grab.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with static features and mins and maxes with timestamps\n",
    "    \"\"\"\n",
    "    # get timeseries for each utility\n",
    "    group_timeseries_elec = df[[\"datapoint_id\", \"Timestamp\", \"energy_elec\"]].groupby('datapoint_id', sort=False)\n",
    "    group_timeseries_gas = df[[\"datapoint_id\", \"Timestamp\", \"energy_gas\"]].groupby('datapoint_id', sort=False)\n",
    "\n",
    "    df_ = df.copy()\n",
    "\n",
    "    # create new columns\n",
    "    for i in range(count):\n",
    "            df_[f\"temax{i}\"] = 0\n",
    "            df_[f\"emax{i}\"] = 0\n",
    "            df_[f\"temin{i}\"] = 0\n",
    "            df_[f\"emin{i}\"] = 0\n",
    "\n",
    "    for t in group_timeseries_elec.groups:\n",
    "        df_ts = group_timeseries_elec.get_group(t)[[\"Timestamp\", \"energy_elec\"]]\n",
    "        ts = df_ts.energy_elec\n",
    "        ts_range = ts.index.copy()\n",
    "        # loop in number of mins and maxes\n",
    "        for i in range(count):\n",
    "            imaxv, maxv = ts.idxmax(), ts.max()\n",
    "            iminv, minv = ts.idxmin(), ts.min()\n",
    "            max_ts = df_ts.loc[imaxv, \"Timestamp\"]\n",
    "            min_ts = df_ts.loc[iminv, \"Timestamp\"]\n",
    "\n",
    "            # this works, datetime object to pandas but it doesnt work on sampling\n",
    "            #df_.loc[ts_range, f\"temax{i}\"] = datetime.strptime(max_ts, '%Y-%m-%d %H:%M:%S')\n",
    "            # so we take a string for the hour\n",
    "            df_.loc[ts_range, f\"temax{i}\"] = str(datetime.strptime(max_ts, '%Y-%m-%d %H:%M:%S').hour)\n",
    "            df_.loc[ts_range, f\"emax{i}\"] = maxv\n",
    "            df_.loc[ts_range, f\"temin{i}\"]= str(datetime.strptime(min_ts, '%Y-%m-%d %H:%M:%S').hour)\n",
    "            df_.loc[ts_range, f\"emin{i}\"] = minv\n",
    "            ts = ts.drop(imaxv)\n",
    "            ts = ts.drop(iminv)\n",
    "\n",
    "    for i in range(count):\n",
    "            df_[f\"tgmax{i}\"] = 0\n",
    "            df_[f\"gmax{i}\"] = 0\n",
    "            df_[f\"tgmin{i}\"] = 0\n",
    "            df_[f\"gmin{i}\"] = 0\n",
    "\n",
    "    for t in group_timeseries_gas.groups:\n",
    "        df_ts = group_timeseries_gas.get_group(t)[[\"Timestamp\", \"energy_gas\"]]\n",
    "        ts = df_ts.energy_gas\n",
    "        ts_range = ts.index.copy()\n",
    "        # Note 1: dont look for minmax if timeseries is flat or zero\n",
    "        # often there is no gas measurements\n",
    "        # Note 2: this doesnt handle all cases, this assumes that if there is a gas measurement\n",
    "        # then there are measurements for each hour.\n",
    "        # Note 3: mins and maxes for gas contributes to sparsity given that a lot of \n",
    "        # buildings do not have gas consumption.\n",
    "        if len(ts.unique()) > 2:\n",
    "            for i in range(count):\n",
    "                imaxv, maxv = ts.idxmax(), ts.max()\n",
    "                iminv, minv = ts.idxmin(), ts.min()\n",
    "                max_ts = df_ts.loc[imaxv, \"Timestamp\"]\n",
    "                min_ts = df_ts.loc[iminv, \"Timestamp\"]\n",
    "                # CPAR model does not like datetime types here, extracting the hour only\n",
    "                df_.loc[ts_range, f\"tgmax{i}\"] = str(datetime.strptime(max_ts, '%Y-%m-%d %H:%M:%S').hour)\n",
    "                df_.loc[ts_range, f\"gmax{i}\"] = maxv\n",
    "                df_.loc[ts_range, f\"tgmin{i}\"]= str(datetime.strptime(min_ts, '%Y-%m-%d %H:%M:%S').hour)\n",
    "                df_.loc[ts_range, f\"gmin{i}\"] = minv\n",
    "                ts = ts.drop(imaxv)\n",
    "                ts = ts.drop(iminv)\n",
    "        else:\n",
    "            for i in range(count):\n",
    "                df_.loc[ts_range, f\"tgmax{i}\"] = str(datetime.strptime(\"2000-01-01 00:00:00\", '%Y-%m-%d %H:%M:%S').hour)\n",
    "                df_.loc[ts_range, f\"gmax{i}\"] = 0\n",
    "                df_.loc[ts_range, f\"tgmin{i}\"]= str(datetime.strptime(\"2000-01-01 00:00:00\", '%Y-%m-%d %H:%M:%S').hour)\n",
    "                df_.loc[ts_range, f\"gmin{i}\"] = 0\n",
    "\n",
    "    # Quick validation, the static emax0 values should be the same as computing\n",
    "    # the max of each time series\n",
    "    #df[[\"datapoint_id\",\"energy_elec\"]].groupby('datapoint_id', sort=False).nlargest(count).values.flatten()\n",
    "    # for i in range(count):\n",
    "    #     if not np.allclose(df_[f\"emax{i}\"], df[[\"datapoint_id\",\"energy_elec\"]].groupby('datapoint_id', sort=False).max().values.flatten()):\n",
    "    #     raise ValueError(\"Max of time series mismatch\")\n",
    "    if not np.allclose(df_.emax0.unique(), df[[\"datapoint_id\",\"energy_elec\"]].groupby('datapoint_id', sort=False).max().values.flatten()):\n",
    "        raise ValueError(\"Max of time series mismatch\")\n",
    "\n",
    "    # force datetime on pandas\n",
    "    #for i in range(count):\n",
    "    #    df_[f\"temax{i}\"] = pd.to_datetime(df_[f\"temax{i}\"])\n",
    "    #     df_[f\"temin{i}\"] = pd.to_datetime(df_[f\"temin{i}\"])       \n",
    "    #     df_[f\"tgmax{i}\"] = pd.to_datetime(df_[f\"tgmax{i}\"])\n",
    "    #     df_[f\"tgmin{i}\"] = pd.to_datetime(df_[f\"tgmin{i}\"])\n",
    "\n",
    "\n",
    "    # let's reorder the columns\n",
    "    cols = list(df_.columns)\n",
    "    lencs = len(cols)\n",
    "    print(f\"Columns in {len(df.columns)}, columns out {lencs}\")\n",
    "    if lencs != len(df.columns) + 8*count:\n",
    "        raise ValueError(\"Input / output columns mismatch\")\n",
    "    \n",
    "    return df_[cols[0:4]+cols[-8*count:]+cols[4:lencs-8*count]]#.drop(columns=[#\"temax0\",\n",
    "                                                                              #\"emax0\", \n",
    "                                                               #               \"temin0\", \"emin0\",\n",
    "                                                               #               \"tgmax0\", \"gmax0\", \"tgmin0\", \"gmin0\"\n",
    "                                                               #              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4065c27-77bc-4b94-adc8-7750177988cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "real_data_mm = pick_peaks(real_data, peaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa45609e-572f-405a-831e-8b97f7cab9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data_mm.to_csv(f\"real_data_sdv_{days}_days_{peaks}_static_peaks.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c2b746-8ad1-4b58-8328-94d8b2eabdfa",
   "metadata": {},
   "source": [
    "#### Manipulate data to conform SDV data flow\n",
    "- Treat every feature with less than 30 unique elements as string to make it categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf944a4a-989f-427f-99c3-d8ac053233fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data = real_data_mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b32a61a-a2cb-4482-9c2d-94c8b50161f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if real_data.isnull().values.any():\n",
    "    raise ValueError(\"Real data has null values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f7dd5b-50c6-443e-a938-923f3797ba7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf804081-5662-4eea-9eab-6d639f06ac32",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for col, dt in real_data.dtypes[4:].items():\n",
    "    if dt == \"float64\" or dt == \"int64\":\n",
    "        if len(real_data[col].unique()) < 30:\n",
    "            real_data[col] = real_data[[col]].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f838261-fd84-4d27-93d0-3c14fd42402f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(real_data.dtypes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff300291-9f2f-4f07-8bb4-7d8a903e2ce3",
   "metadata": {},
   "source": [
    "### Define and handle metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf4e8ed-fc6b-456d-8375-b1c7ad9896df",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data.temax0.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a831088-9e8c-4bc4-ba8f-dfa684a6bb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = sdv.metadata.SingleTableMetadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce2d363-6bb1-42e3-ba86-07689d9c50aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.detect_from_dataframe(real_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee573d3e-7120-4f53-b953-cbbd597684bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da258d81-e1f8-484f-bf39-cf90d57de3b9",
   "metadata": {},
   "source": [
    "#### Adjusting metadata for timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af329e3-1603-4018-a58a-695106313f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update metadata for datapoint_id, set to id and hex string to avoid duplications\n",
    "metadata.update_column(column_name='datapoint_id', sdtype='id', regex_format='[0-9a-f]{6}')#, regex='[0-9a-f]{32}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3a9ba8-0b6f-4d3c-ad3b-8fa7652ca7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(peaks):\n",
    "#    metadata.update_column(column_name=f\"temax{i}\", sdtype='datetime', datetime_format='%Y-%m-%d %H:%M:%S')\n",
    "#     metadata.update_column(column_name=f\"temin{i}\", sdtype='datetime', datetime_format='%Y-%m-%d %H:%M:%S')\n",
    "#     metadata.update_column(column_name=f\"tgmax{i}\", sdtype='datetime', datetime_format='%Y-%m-%d %H:%M:%S')\n",
    "#     metadata.update_column(column_name=f\"tgmin{i}\", sdtype='datetime', datetime_format='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7f9f22-e1f2-46fe-ab72-f7bee4056274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each datapoint_id (unique) holds a timeseries\n",
    "metadata.set_sequence_key(column_name='datapoint_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e900f3c-8656-420e-ad87-f9c13aca3df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The index for timeseries is the Timestamp\n",
    "metadata.set_sequence_index(column_name='Timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e9d617-88a4-44b2-a0e5-efd7177d59b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd86354-6493-41bb-965b-765e528b238f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1016c474-fed2-48c0-a998-731423c2a877",
   "metadata": {},
   "source": [
    "#### Adapt the input data to the model\n",
    "- Set context columns (features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf54359-4ff9-492f-91e5-fdd463f03f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a29f0dc-ff73-4ae5-b7a7-67f7c162fdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_cols = list(real_data.columns)[4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16384d15-076f-44b5-80ff-b69c10b1c74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3431cca9-b4f4-4faf-a3ac-2a4f1c73ffc8",
   "metadata": {},
   "source": [
    "## Fit synthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efb360a-f11d-445f-8ff4-80ca01800919",
   "metadata": {},
   "outputs": [],
   "source": [
    "#epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36dfb8b-ef6a-44d0-a751-48d163384513",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesizer = PARSynthesizer(\n",
    "    metadata,\n",
    "    context_columns=context_cols,\n",
    "    verbose=True,\n",
    "    epochs=epochs)\n",
    "    #segment_size=7)#128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deda355d-07e8-44a5-9ed4-1dde2c1184dd",
   "metadata": {},
   "source": [
    "|days| RAM| VRAM| Epochs|Fit| Gen|\n",
    "|---|---|---|---|---|---|\n",
    "|1|~3.5Gb|~0.6Gb| 10|~3min|~3min|\n",
    "|1|~3.5Gb|~0.6Gb| 128|~42min|~9min|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2c532b-da31-47ee-a15e-5635a82a4494",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "synthesizer.fit(real_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fb292c-d607-4766-bb86-a145d7a78740",
   "metadata": {},
   "source": [
    "## Sample SD and save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a4fa57-b6e6-4010-be05-ca80d8bb9dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "synthetic_data = synthesizer.sample(num_sequences=sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed4e07a-f5c4-470e-baf1-2888b8988c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24339255-4eca-44dd-a893-409dcd3ce324",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cacc6eb-d1f3-4199-8157-8cde7e40f4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46eaad5c-652c-43c4-840e-8cb9a4a9ef6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data[:24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3da7eda-af8b-4ef9-b9ec-9f761825b693",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7278f0bf-ec66-4b01-a8a5-5b7d44693120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "synthesizer.save(f'quick_test_PAR_full_cols_{days}_days_{peaks}_static_peaks.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53527b3-84be-4fc0-8607-9b5a3d2b4926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump data\n",
    "synthetic_data.to_csv(f\"synthetic_data_sdv_{days}_days_{peaks}_static_peaks.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb5c88b-213b-4541-82e1-be368bdd8a4c",
   "metadata": {},
   "source": [
    "## Done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
