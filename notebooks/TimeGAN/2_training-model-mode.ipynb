{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "162a5122-41d1-45f3-a81d-7c93694b74fa",
   "metadata": {},
   "source": [
    "# Synthcity Time Series generator training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3ac0dc-6820-4017-b126-f5aad6354c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sometimes we have to purge the workspace to avoid errors\n",
    "!rm -rf workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848c3e0a-809d-478f-8eb3-a86933b11599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stdlib\n",
    "import sys\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# synthcity absolute\n",
    "import synthcity.logger as log\n",
    "from synthcity.plugins import Plugins\n",
    "from synthcity.plugins.core.dataloader import TimeSeriesDataLoader\n",
    "from synthcity.benchmark import Benchmarks\n",
    "from synthcity.utils.serialization import load, load_from_file, save, save_to_file\n",
    "from synthcity.plugins.core.constraints import Constraints\n",
    "\n",
    "log.add(sink=sys.stderr, level=\"INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9e5c36-df4b-43d3-939e-a3b3639d0c14",
   "metadata": {},
   "source": [
    "### Inputs\n",
    "- Number of days or datafilename\n",
    "- Number of iteration / epochs\n",
    "- Size of sampled synthetic data\n",
    "- Real data file name\n",
    "- Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69099c84-1e06-4d40-ac23-f6a2de7d6cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "days = 1\n",
    "data_dir = \"../\"\n",
    "n_iter = 100# 1000 is the default\n",
    "sample_size = None\n",
    "datafilename = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe9daa0-b631-4feb-b196-ecbd30d46022",
   "metadata": {},
   "outputs": [],
   "source": [
    "Plugins(categories=[\"time_series\"]).list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9475f2b5-ed4b-4619-b777-c136031e2907",
   "metadata": {},
   "outputs": [],
   "source": [
    "generators = Plugins(categories=[\"time_series\"]).list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3996989-e3ac-4a27-a981-1b51a7e72b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = \"timegan\"\n",
    "mode = \"GRU\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361208ed-be44-4279-bd98-47581dc3dfac",
   "metadata": {},
   "source": [
    "### Read real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37f15e4-f6d5-491d-b8d2-5b95cda32b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "if datafilename:\n",
    "    real_data = pd.read_csv(datafilename, index_col=0)    \n",
    "else:\n",
    "    real_data = pd.read_csv(f\"{data_dir}real_data_sdv_{days}_days.csv\", index_col=0)\n",
    "\n",
    "if not sample_size:\n",
    "    sample_size = len(real_data.datapoint_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042da51a-73e8-4133-9a3b-6a7f3b4ca401",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c86233-3682-4910-a101-9b07def65368",
   "metadata": {},
   "source": [
    "### Extract time series and instantiate TimeSeriesDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0ef993-ed7d-4d54-90b7-ca05d3b08a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ts(df):\n",
    "    \"\"\" Extract time series for each `datapoint_id`\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe with static and time series values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with static features\n",
    "    \n",
    "    List\n",
    "        List of time series DataFrames\n",
    "    \"\"\"\n",
    "    # get static features\n",
    "    static_df = df.drop(columns=[\"Timestamp\", \"energy_elec\", \"energy_gas\"]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # get timeseries for each datapoint_id\n",
    "    group_timeseries = df[[\"datapoint_id\", \"Timestamp\", \"energy_elec\", \"energy_gas\"]].groupby('datapoint_id', sort=False)\n",
    "    # timeseries_dfs = [group_timeseries.get_group(t)[[#\"Timestamp\",\n",
    "    #                                                  \"energy_elec\",\n",
    "    #                                                  \"energy_gas\"]] for t in group_timeseries.groups]    \n",
    "\n",
    "    timeseries_dfs = []\n",
    "    for t in group_timeseries.groups:\n",
    "        # WARNING, FIXME for now avoid timestamps and work with hours as index\n",
    "        tdf = group_timeseries.get_group(t)[[#\"Timestamp\",\n",
    "                                             \"energy_elec\",\n",
    "                                             \"energy_gas\"]]\n",
    "        # ts_df[\"Timestamp\"] = ts_df[\"Timestamp\"].apply(lambda t: datetime.strptime(t, '%Y-%m-%d %H:%M:%S').hour)\n",
    "        # ts_df = ts_df.set_index(\"Timestamp\")\n",
    "\n",
    "        # so now drop consecutive indices and reset it to 0 to 23\n",
    "        tdf = tdf.reset_index(drop=True)\n",
    "        tdf.index.name=\"hour\"\n",
    "        timeseries_dfs.append(tdf)\n",
    "\n",
    "    if len(timeseries_dfs) != len(static_df):\n",
    "        raise ValueError(f\"Number of datapoint_ids {len(static_df)} doesn't match the number of time series {len(timeseries_dfs)}\")\n",
    "    \n",
    "    return static_df, timeseries_dfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ac6709-8d7b-4d30-af99-9151424dc2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "static_df, timeseries_dfs = extract_ts(real_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adba857-2f3a-43ef-ab68-225e5e5ef092",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(f\"timseries_dfs_synthcity_{days}_days.pkl\", \"wb\") as f:\n",
    "#    pickle.dump(timeseries_dfs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e2bf0d-a64a-4781-b874-017757741f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#static_df.to_csv(f\"static_data_synthcity_{days}_days.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57289a9f-d322-4cf0-8e64-495037e532ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates the list of observation times, usually 0 to 24 hours x 4000\n",
    "observation_data = []\n",
    "#outcome = []\n",
    "for tdf in timeseries_dfs:\n",
    "    observations = list(tdf.index)\n",
    "    observation_data.append(observations)\n",
    "    #outcome.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832e1419-f30e-4b35-b030-7c9cb344b415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fake the outcome, we dont have/know the ML task for this dataset so let's fake the target\n",
    "outcome = np.random.randint(2, size=(len(static_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a315f09-5283-44cc-9235-fcc8a0f9bd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_df = pd.DataFrame(outcome, columns=[\"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456996ec-55f5-41f8-8fc7-361ec3ab7ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work with a subset of the features since the code breaks a lot!!!\n",
    "static_data = static_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2540da0-4c4a-4921-9ed4-de2def8795ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81086c0d-b630-4387-b2be-c79a0a1e67e7",
   "metadata": {},
   "source": [
    "#### Treat every feature with less than 30 unique elements as string to make it categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9e6bb4-559a-4a80-8a24-33926f64b0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for col, dt in static_data.dtypes.items():\n",
    "    if dt == \"float64\" or dt == \"int64\":\n",
    "        if len(static_data[col].unique()) < 30:\n",
    "            static_data[col] = static_data[[col]].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00009d18-6004-4511-8421-e66c670b1391",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1b237c-a3ba-4122-b59e-e259962c2f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_dfs[2].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0878a8-a28e-46c6-a560-d8357854539b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"real_timeseries_dfs_synthcity_{days}_days.pkl\", \"wb\") as f:\n",
    "    pickle.dump(timeseries_dfs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91378873-5b67-4726-bc96-b4edd791316d",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_data.drop(columns=[\"datapoint_id\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0692ca9b-6fce-48b4-ac45-e80b689d7b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_data.to_csv(f\"real_static_data_synthcity_{days}_days.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69a701c-0323-44b7-b251-663088eb96d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# instantiate time series data loader\n",
    "loader = TimeSeriesDataLoader(\n",
    "    temporal_data=timeseries_dfs,\n",
    "    observation_times=observation_data,\n",
    "    static_data=static_data,\n",
    "    outcome=outcome_df,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341d2a40-8d5c-41a1-914c-eda46f32105a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9b4300-8f06-4700-9924-6bbb1d7dc8d8",
   "metadata": {},
   "source": [
    "## Train generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350a5570-b732-4ea8-ac21-2872decf3f51",
   "metadata": {},
   "source": [
    "Available \"modes\" for TimeGAN:\n",
    "```\n",
    "mode: str = \"RNN\"\n",
    "    Core neural net architecture.\n",
    "    Available models:\n",
    "        - \"LSTM\"\n",
    "        - \"GRU\"\n",
    "        - \"RNN\"\n",
    "        - \"Transformer\"\n",
    "        - \"MLSTM_FCN\"\n",
    "        - \"TCN\"\n",
    "        - \"InceptionTime\"\n",
    "        - \"InceptionTimePlus\"\n",
    "        - \"XceptionTime\"\n",
    "        - \"ResCNN\"\n",
    "        - \"OmniScaleCNN\"\n",
    "        - \"XCM\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a19a61b-0c7d-4301-b09e-a9bcc9196812",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"tsdloader_synthcity_{days}_days.pkl\", \"wb\") as f:\n",
    "    pickle.dump(loader, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3e4647-87e3-475d-90f9-ef37c8496f74",
   "metadata": {},
   "source": [
    "|mode| RAM| VRAM| Epochs|Fit| Gen|\n",
    "|---|---|---|---|---|---|\n",
    "|1|~14Gb|~900Mb| 50|~13min|~10min|\n",
    "|1|~14Gb|~900Mb| 100|~22min|~22min|\n",
    "|1|~14Gb|~900Mb| 150|~31min|~22min|\n",
    "|1|~14Gb|~900Mb| 1000|~3h15min|~15min|\n",
    "|*LSTM|~14Gb|~1Gb| 100|~28min|~15min|\n",
    "|*GRU|~14Gb|~1Gb| 100|~28min|~8min|\n",
    "\n",
    "*VRAM on generation for 1K samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d9e6fe-9094-4759-83d9-962286dc48d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "syn_model = Plugins().get(\"timegan\",\n",
    "                          n_iter=n_iter,\n",
    "                          mode=mode)\n",
    "syn_model.fit(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd50dae-86d9-41f8-b8d5-27bc0613b3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's save the generator\n",
    "save_to_file(f\"model_{syn_model.name()}_mode_{mode}_synthcity_days_{days}_niter_{syn_model.n_iter}.pkl\", syn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a55726a-6670-43ff-903c-a6faaf56799d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "synthetic_data = []\n",
    "batch_size = 1000\n",
    "all_ids = set()\n",
    "count, lcount = 0, 0\n",
    "iter = 0\n",
    "if not sample_size:\n",
    "    sample_size = syn_model.data_info['len']\n",
    "\n",
    "while len(synthetic_data) < syn_model.data_info['len']:\n",
    "    lsd = len(synthetic_data)\n",
    "    sd_dfs = []\n",
    "    sd_df = syn_model.generate(batch_size, sampling_patience=1000).dataframe()\n",
    "    generated = len(sd_df[\"seq_id\"].unique())\n",
    "    #print(f\"total samples generated {len(sd_df)}, uniques {generated}\")\n",
    "\n",
    "    for id, tidf in sd_df.groupby(\"seq_id\", sort=False):\n",
    "        if len(tidf) == 24:\n",
    "            df = tidf.sort_values(by=[\"seq_time_id\"]).copy()\n",
    "            df[\"seq_time_id\"] = list(range(24))\n",
    "            sd_dfs.append(df)\n",
    "        else:\n",
    "            lcount += len(tidf)\n",
    "            count += 1\n",
    "\n",
    "    if len(sd_dfs) > 0:\n",
    "        if lsd == 0:\n",
    "            synthetic_data = pd.concat(sd_dfs)\n",
    "            seq_ids = np.repeat(np.arange(0, len(synthetic_data[\"seq_id\"].unique())), 24)\n",
    "            #print(f\"total samples {len(synthetic_data)}, {len(seq_ids)}\")\n",
    "            synthetic_data[\"seq_id\"] = seq_ids\n",
    "        else:\n",
    "            #sd_dfs.insert(0, synthetic_data)\n",
    "            sd_new_df = pd.concat(sd_dfs)\n",
    "            uids_len_start = len(synthetic_data[\"seq_id\"].unique())+1\n",
    "            uids_len_stop = uids_len_start + len(sd_new_df[\"seq_id\"].unique())\n",
    "            seq_ids = np.arange(uids_len_start, uids_len_stop)\n",
    "            #print(f\"old samples {len(synthetic_data)}, new samples {len(sd_df)}\")\n",
    "            sd_new_df[\"seq_id\"] = np.repeat(seq_ids, 24)\n",
    "            synthetic_data = pd.concat([synthetic_data, sd_new_df])        \n",
    "    iter += 1\n",
    "    print(f\"total samples {len(synthetic_data)}, generated in this step <{len(synthetic_data)-lsd}>, pct <{100*(len(synthetic_data)-lsd)/(len(sd_df)):.2f}%> total bad samples {lcount} -> {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c151e0ac-e7a2-4ebe-a2d2-391bd6160e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data = synthetic_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cdf0a2-49f6-422e-9319-49336139839c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get the exact number of samples, this could be improved by sampling on a larger sample size\n",
    "# but have to consider based in groups of seq_ids\n",
    "synthetic_data = synthetic_data[:syn_model.data_info[\"len\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344b0ce1-d5e4-4081-8f38-275ce6a7a18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ad0898-8967-4443-b92e-96aca6091dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data.to_csv(f\"synthetic_data_model_{syn_model.name()}_mode_{mode}_synthcity_days_{days}_niter_{syn_model.n_iter}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2541245b-d1c4-4536-9e7c-1f2d79856f0d",
   "metadata": {},
   "source": [
    "## Done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
